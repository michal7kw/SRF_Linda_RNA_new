import anndata
import os
import scipy.sparse
import math
import logging
import pandas as pd # Needed for subsetting check

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Configuration ---
# Input file is the one generated by raw_5_finalize_annotation.py
INPUT_H5AD_PATH = os.path.join('results_from_raw', 'final_annotation', 'merged_raw_final_annotated_simple.h5ad')

# OUTPUT_H5AD_PATH_BASE = 'merged_raw_subset_2_28_15_compressed' # Base name for SUBSET output file(s)
# OUTPUT_H5AD_PATH_BASE = 'merged_raw_subset_14_25_26_compressed' # Base name for SUBSET output file(s)
OUTPUT_H5AD_PATH_BASE = 'merged_raw_subset_2_28_compressed' # Base name for SUBSET output file(s)

OUTPUT_DIR = os.path.join('results_from_raw', 'MapMyCells', OUTPUT_H5AD_PATH_BASE)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# CLUSTERS_TO_KEEP = ['2', '28', '15']
# CLUSTERS_TO_KEEP = ['14', '25', '26'] 
CLUSTERS_TO_KEEP = ['2', '28']

CLUSTERING_KEY = 'leiden_0.8_alt'
 
MAX_FILE_SIZE_GB = 2
MAX_FILE_SIZE_BYTES = MAX_FILE_SIZE_GB * 1024 * 1024 * 1024 # 2 GB in bytes

# --- PART 1: DATA INPUT & SUBSETTING ---
logging.info(f"Loading full AnnData object from: {INPUT_H5AD_PATH}")
try:
    adata_full = anndata.read_h5ad(INPUT_H5AD_PATH)
    logging.info("Full AnnData object loaded successfully.")
    logging.info(f"Original shape: {adata_full.shape}")
except FileNotFoundError:
    logging.error(f"Error: Input file not found at {INPUT_H5AD_PATH}")
    exit()
except Exception as e:
    logging.error(f"An error occurred while loading the AnnData object: {e}")
    exit()

# Subset the data
logging.info(f"Subsetting data to keep clusters {CLUSTERS_TO_KEEP} from '{CLUSTERING_KEY}'")

# Check if the clustering key exists
if CLUSTERING_KEY not in adata_full.obs.columns:
    logging.error(f"Error: Clustering key '{CLUSTERING_KEY}' not found in adata.obs")
    logging.info(f"Available obs columns: {list(adata_full.obs.columns)}")
    exit()

try:
    # Ensure consistent type for comparison (assume clusters are strings based on input)
    mask = adata_full.obs[CLUSTERING_KEY].astype(str).isin(CLUSTERS_TO_KEEP)
    adata_subset = adata_full[mask].copy() # Use copy() to avoid views

    if adata_subset.shape[0] == 0:
        logging.warning(f"Warning: No cells found matching clusters {CLUSTERS_TO_KEEP} in '{CLUSTERING_KEY}'. Output file will be empty or script might fail.")
        # Decide whether to exit or proceed with empty data
        # exit()
    else:
         logging.info(f"Subset created successfully. New shape: {adata_subset.shape}")

except Exception as e:
    logging.error(f"An error occurred during subsetting: {e}")
    exit()

# Extract necessary components from the SUBSET
count_matrix = adata_subset.X
obs_data = adata_subset.obs # Assuming index contains cell IDs
var_data = adata_subset.var # Assuming index contains gene IDs

# --- PART 2: DATA OUTPUT (using the subset) ---

logging.info("Ensuring subset count matrix is in CSR format.")
# Convert count matrix to CSR sparse format if it isn't already
if not isinstance(count_matrix, scipy.sparse.csr_matrix):
    try:
        count_matrix = scipy.sparse.csr_matrix(count_matrix)
        logging.info("Converted subset count matrix to CSR format.")
    except Exception as e:
        logging.error(f"Failed to convert subset matrix to CSR format: {e}")
        exit()
else:
    logging.info("Subset count matrix is already in CSR format.")

# Create the AnnData object for saving (using the subset data)
try:
    # Use copies of the subset's obs/var to be safe
    ad_to_save = anndata.AnnData(
        X=count_matrix,
        obs=obs_data.copy(),
        var=var_data.copy()
    )
    # Optionally copy essential .uns, .obsm etc. from the subset if needed for MapMyCells
    # ad_to_save.uns = adata_subset.uns.copy() # Be selective if needed
    # ad_to_save.obsm = adata_subset.obsm.copy()
    logging.info("Created AnnData object from subset for saving.")
except Exception as e:
    logging.error(f"Failed to create AnnData object from subset for saving: {e}")
    exit()

# Define the initial output path for the subset
output_path_single = os.path.join(OUTPUT_DIR, f"{OUTPUT_H5AD_PATH_BASE}.h5ad")

# Write the full subset data to a compressed file initially
logging.info(f"Writing compressed SUBSET AnnData object to: {output_path_single}")
try:
    ad_to_save.write_h5ad(output_path_single, compression='gzip')
    logging.info("Compressed subset file written successfully.")
except Exception as e:
    logging.error(f"Failed to write compressed subset h5ad file: {e}")
    exit()

# Determine the file size of the compressed subset
try:
    file_size_bytes = os.path.getsize(output_path_single)
    file_size_gb = file_size_bytes / (1024 * 1024 * 1024)
    logging.info(f"Compressed subset file size: {file_size_bytes} bytes ({file_size_gb:.2f} GB)")
except FileNotFoundError:
    logging.error(f"Error: Could not find the written subset file {output_path_single} to check its size.")
    exit()

# Check if file size exceeds the limit
if file_size_bytes > MAX_FILE_SIZE_BYTES:
    logging.info(f"Subset file size exceeds {MAX_FILE_SIZE_GB}GB limit. Splitting into multiple files.")

    # Determine the number of files needed and rows per file for the SUBSET
    num_files_needed = math.ceil(file_size_bytes / MAX_FILE_SIZE_BYTES)
    num_total_rows = ad_to_save.shape[0] # Use shape of the subset
    rows_per_subset_chunk = num_total_rows // num_files_needed

    logging.info(f"Splitting subset into {num_files_needed} files with approximately {rows_per_subset_chunk} rows each.")
    if num_files_needed > 8:
        logging.warning("Splitting subset into more than 8 files. Consider using a code-based mapping approach if possible.")

    # Write each chunk to a separate file
    split_success = True
    for i in range(num_files_needed):
        start_idx = i * rows_per_subset_chunk
        # Ensure the last chunk includes any remaining rows
        end_idx = (i + 1) * rows_per_subset_chunk if i < num_files_needed - 1 else num_total_rows

        output_path_part = os.path.join(OUTPUT_DIR, f"{OUTPUT_H5AD_PATH_BASE}_part_{i+1}_of_{num_files_needed}.h5ad")
        logging.info(f"Writing subset part {i+1}/{num_files_needed} (rows {start_idx} to {end_idx}) to {output_path_part}")

        try:
            # Create AnnData object for the chunk using the already prepared CSR matrix and sliced obs
            ad_chunk = anndata.AnnData(
                X=count_matrix[start_idx:end_idx, :], # Slice the CSR matrix of the subset
                obs=obs_data.iloc[start_idx:end_idx].copy(), # Slice the obs dataframe of the subset
                var=var_data.copy() # Var remains the same for all chunks
            )
            # Write the chunk with compression
            ad_chunk.write_h5ad(output_path_part, compression='gzip')
            logging.info(f"Successfully wrote subset part {output_path_part}")
        except Exception as e:
            logging.error(f"Failed to write subset chunk {i+1} to {output_path_part}: {e}")
            split_success = False
            break # Stop splitting if one part fails

    if split_success:
        logging.info("Successfully split the subset data into multiple files.")
        # Optionally remove the original large subset file if splitting was successful
        logging.info(f"Removing the original large subset file: {output_path_single}")
        try:
            os.remove(output_path_single)
        except OSError as e:
            logging.warning(f"Could not remove original large subset file {output_path_single}: {e}")
    else:
        logging.error("Subset file splitting failed. The original large subset file was kept.")

else:
    logging.info(f"Subset file size is within the {MAX_FILE_SIZE_GB}GB limit. No splitting needed.")
    logging.info(f"Final subset output file: {output_path_single}")

logging.info("Script finished.")
